{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369477d2",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Tutorial\n",
    "## House Price Prediction Using Multiple Features\n",
    "\n",
    "Welcome to **Multiple Linear Regression**! This builds upon simple linear regression by using multiple features to make predictions.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to use multiple features (area, bedrooms, age) simultaneously\n",
    "- Feature scaling and its importance in multiple regression\n",
    "- Correlation analysis between multiple variables\n",
    "- How to interpret coefficients in multiple regression\n",
    "- Comparison with simple linear regression performance\n",
    "\n",
    "### Key Concepts:\n",
    "- **Multiple features**: Using more than one input variable\n",
    "- **Feature scaling**: Standardizing features for better performance\n",
    "- **Multicollinearity**: When features are correlated with each other\n",
    "- **Feature importance**: Understanding which features matter most\n",
    "\n",
    "Let's dive into multiple linear regression! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a696",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data\n",
    "\n",
    "We'll need additional preprocessing tools for multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler  # New: for feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\"‚úÖ Libraries imported and data loaded!\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a798e",
   "metadata": {},
   "source": [
    "## Step 2: Explore the Dataset\n",
    "\n",
    "Let's understand our data better by examining relationships between multiple features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22894751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure and basic statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Features: {list(data.columns)}\")\n",
    "print(f\"Data types:\\n{data.dtypes}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\\n{data.isnull().sum()}\")\n",
    "\n",
    "# Basic statistical summary\n",
    "print(f\"\\nStatistical Summary:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check target variable distribution\n",
    "print(f\"\\nTarget Variable (Price) Analysis:\")\n",
    "print(f\"Mean price: ${data['price'].mean():,.2f}\")\n",
    "print(f\"Median price: ${data['price'].median():,.2f}\")\n",
    "print(f\"Price range: ${data['price'].min():,.2f} - ${data['price'].max():,.2f}\")\n",
    "print(f\"Price std: ${data['price'].std():,.2f}\")\n",
    "\n",
    "# Unique values for categorical features\n",
    "categorical_features = ['location']\n",
    "for feature in categorical_features:\n",
    "    if feature in data.columns:\n",
    "        print(f\"\\n{feature.capitalize()} unique values: {data[feature].nunique()}\")\n",
    "        print(f\"{feature.capitalize()} values: {data[feature].unique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce396e",
   "metadata": {},
   "source": [
    "## Step 3: Correlation Analysis\n",
    "\n",
    "Analyze how different features correlate with each other and with the target variable (price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between features and target\n",
    "print(\"=\" * 50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_features = ['area', 'bedrooms', 'age', 'price']\n",
    "correlation_data = data[numerical_features]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Correlation with target variable (price)\n",
    "price_correlations = correlation_matrix['price'].sort_values(ascending=False)\n",
    "print(f\"\\nCorrelations with Price (sorted):\")\n",
    "for feature, corr in price_correlations.items():\n",
    "    if feature != 'price':\n",
    "        strength = \"\"\n",
    "        if abs(corr) > 0.7:\n",
    "            strength = \"Strong\"\n",
    "        elif abs(corr) > 0.3:\n",
    "            strength = \"Moderate\"\n",
    "        else:\n",
    "            strength = \"Weak\"\n",
    "        \n",
    "        direction = \"Positive\" if corr > 0 else \"Negative\"\n",
    "        print(f\"  {feature:10s}: {corr:7.3f} ({strength} {direction})\")\n",
    "\n",
    "# Create visualizations for correlations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Multiple Feature Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Correlation heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Correlation Heatmap')\n",
    "\n",
    "# 2. Scatter plots of features vs price\n",
    "features_to_plot = ['area', 'bedrooms', 'age']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (feature, color) in enumerate(zip(features_to_plot, colors)):\n",
    "    if i == 0:\n",
    "        ax = axes[0, 1]\n",
    "    elif i == 1:\n",
    "        ax = axes[1, 0]\n",
    "    else:\n",
    "        ax = axes[1, 1]\n",
    "    \n",
    "    ax.scatter(data[feature], data['price'], alpha=0.6, color=color)\n",
    "    ax.set_xlabel(f'{feature.capitalize()}')\n",
    "    ax.set_ylabel('Price ($)')\n",
    "    ax.set_title(f'{feature.capitalize()} vs Price')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(data[feature], data['price'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(data[feature], p(data[feature]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for multicollinearity\n",
    "print(f\"\\nüîç Multicollinearity Check:\")\n",
    "print(\"Strong correlations between features (> 0.7 or < -0.7):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7 and correlation_matrix.columns[i] != 'price' and correlation_matrix.columns[j] != 'price':\n",
    "            print(f\"  {correlation_matrix.columns[i]} & {correlation_matrix.columns[j]}: {corr_value:.3f}\")\n",
    "\n",
    "print(\"If no strong correlations listed above, multicollinearity is not a major concern.\")\n",
    "print(\"\\n‚úÖ Correlation analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92dd04",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation for Multiple Linear Regression\n",
    "\n",
    "Prepare our features and target variable, handling categorical variables and feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b54c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multiple linear regression\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Handle categorical variables (encode location if present)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_processed = data.copy()\n",
    "\n",
    "# Encode categorical features\n",
    "if 'location' in data_processed.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data_processed['location_encoded'] = label_encoder.fit_transform(data_processed['location'])\n",
    "    print(\"Location encoding:\")\n",
    "    for i, location in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {location} -> {i}\")\n",
    "    feature_columns = ['area', 'bedrooms', 'age', 'location_encoded']\n",
    "else:\n",
    "    feature_columns = ['area', 'bedrooms', 'age']\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data_processed[feature_columns]\n",
    "y = data_processed['price']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Features used: {feature_columns}\")\n",
    "\n",
    "# Display feature statistics before scaling\n",
    "print(f\"\\nFeature Statistics (before scaling):\")\n",
    "print(X.describe())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nData split completed:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Training set ratio: {X_train.shape[0] / len(X):.1%}\")\n",
    "\n",
    "# Feature scaling (important for multiple regression)\n",
    "print(f\"\\nüîß Applying Feature Scaling...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nFeature Statistics (after scaling):\")\n",
    "print(X_train_scaled.describe())\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfc038",
   "metadata": {},
   "source": [
    "## Step 5: Train Multiple Linear Regression Model\n",
    "\n",
    "Train our model using all available features and analyze the learned coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the multiple linear regression model\n",
    "print(\"=\" * 50)\n",
    "print(\"MULTIPLE LINEAR REGRESSION TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "\n",
    "# Extract model parameters\n",
    "intercept = model.intercept_\n",
    "coefficients = model.coef_\n",
    "\n",
    "print(f\"\\nüìä Model Parameters:\")\n",
    "print(f\"Intercept (b‚ÇÄ): ${intercept:,.2f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feature, coef in zip(feature_columns, coefficients):\n",
    "    print(f\"  {feature:15s} (Œ≤): ${coef:10,.2f}\")\n",
    "\n",
    "# Create the mathematical equation\n",
    "equation = f\"Price = ${intercept:,.2f}\"\n",
    "for feature, coef in zip(feature_columns, coefficients):\n",
    "    sign = \"+\" if coef >= 0 else \"\"\n",
    "    equation += f\" {sign} ${coef:,.2f} √ó {feature}\"\n",
    "\n",
    "print(f\"\\nüìù Mathematical Model:\")\n",
    "print(f\"  {equation}\")\n",
    "\n",
    "# Interpret coefficients\n",
    "print(f\"\\nüîç Coefficient Interpretation:\")\n",
    "for feature, coef in zip(feature_columns, coefficients):\n",
    "    if 'area' in feature:\n",
    "        unit = \"sq ft\"\n",
    "        interpretation = f\"Each additional {unit} increases price by ${coef:,.2f}\"\n",
    "    elif 'bedrooms' in feature:\n",
    "        unit = \"bedroom\"\n",
    "        interpretation = f\"Each additional {unit} changes price by ${coef:,.2f}\"\n",
    "    elif 'age' in feature:\n",
    "        unit = \"year of age\"\n",
    "        interpretation = f\"Each additional {unit} changes price by ${coef:,.2f}\"\n",
    "    elif 'location' in feature:\n",
    "        interpretation = f\"Location encoding impact: ${coef:,.2f}\"\n",
    "    else:\n",
    "        interpretation = f\"Impact: ${coef:,.2f}\"\n",
    "    \n",
    "    print(f\"  {feature:15s}: {interpretation}\")\n",
    "\n",
    "# Feature importance (absolute values of coefficients)\n",
    "feature_importance = list(zip(feature_columns, np.abs(coefficients)))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nüìà Feature Importance (by coefficient magnitude):\")\n",
    "for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "    print(f\"  {i}. {feature:15s}: {importance:10,.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4faffbf",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions and Evaluate Model\n",
    "\n",
    "Use our trained model to make predictions and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the trained model\n",
    "print(\"=\" * 50)\n",
    "print(\"PREDICTIONS AND EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"‚úÖ Predictions completed!\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nüìä Model Performance Metrics:\")\n",
    "print(f\"{'Metric':20s} {'Training':>12s} {'Testing':>12s}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Mean Squared Error':20s} ${train_mse:11,.0f} ${test_mse:11,.0f}\")\n",
    "print(f\"{'Root Mean Sq Error':20s} ${train_rmse:11,.0f} ${test_rmse:11,.0f}\")\n",
    "print(f\"{'Mean Absolute Error':20s} ${train_mae:11,.0f} ${test_mae:11,.0f}\")\n",
    "print(f\"{'R¬≤ Score':20s} {train_r2:11.4f} {test_r2:11.4f}\")\n",
    "\n",
    "# Model performance interpretation\n",
    "print(f\"\\nüí° Performance Interpretation:\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {test_r2:.1%} of price variation is explained by our features\")\n",
    "print(f\"  ‚Ä¢ RMSE: On average, predictions are off by ${test_rmse:,.0f}\")\n",
    "print(f\"  ‚Ä¢ MAE: Typical prediction error is ${test_mae:,.0f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting_check = train_r2 - test_r2\n",
    "if overfitting_check > 0.1:\n",
    "    print(f\"  ‚ö†Ô∏è Potential overfitting detected (training R¬≤ much higher than test R¬≤)\")\n",
    "elif overfitting_check > 0.05:\n",
    "    print(f\"  üëÄ Slight overfitting (training R¬≤ > test R¬≤)\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Good generalization (similar training and test performance)\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\nüìã Sample Predictions (first 10):\")\n",
    "print(f\"{'Actual':>10s} {'Predicted':>12s} {'Error':>10s} {'Error %':>10s}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(10, len(y_test))):\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = y_test_pred[i]\n",
    "    error = abs(actual - predicted)\n",
    "    error_pct = (error / actual) * 100\n",
    "    \n",
    "    print(f\"${actual:9,.0f} ${predicted:11,.0f} ${error:9,.0f} {error_pct:8.1f}%\")\n",
    "\n",
    "# Calculate prediction accuracy (within certain percentage)\n",
    "def accuracy_within_percent(actual, predicted, percent):\n",
    "    errors = np.abs((actual - predicted) / actual * 100)\n",
    "    return np.mean(errors <= percent) * 100\n",
    "\n",
    "acc_5 = accuracy_within_percent(y_test, y_test_pred, 5)\n",
    "acc_10 = accuracy_within_percent(y_test, y_test_pred, 10)\n",
    "acc_20 = accuracy_within_percent(y_test, y_test_pred, 20)\n",
    "\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "print(f\"  Within 5% of actual price:  {acc_5:.1f}% of predictions\")\n",
    "print(f\"  Within 10% of actual price: {acc_10:.1f}% of predictions\")  \n",
    "print(f\"  Within 20% of actual price: {acc_20:.1f}% of predictions\")\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94680b1b",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results\n",
    "\n",
    "Create comprehensive visualizations to understand our model's performance and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Multiple Linear Regression - Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Actual vs Predicted prices\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, color='blue')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Price ($)')\n",
    "axes[0, 0].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 0].set_title('Actual vs Predicted Prices')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add R¬≤ annotation\n",
    "axes[0, 0].text(0.05, 0.95, f'R¬≤ = {test_r2:.3f}', transform=axes[0, 0].transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "\n",
    "# 2. Residuals plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, color='green')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Price ($)')\n",
    "axes[0, 1].set_ylabel('Residuals ($)')\n",
    "axes[0, 1].set_title('Residuals Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature importance\n",
    "feature_names_clean = [name.replace('_encoded', '') for name in feature_columns]\n",
    "importance_values = np.abs(coefficients)\n",
    "colors = ['blue', 'green', 'red', 'orange'][:len(feature_columns)]\n",
    "\n",
    "bars = axes[0, 2].bar(range(len(feature_columns)), importance_values, \n",
    "                      alpha=0.7, color=colors)\n",
    "axes[0, 2].set_xlabel('Features')\n",
    "axes[0, 2].set_ylabel('Coefficient Magnitude')\n",
    "axes[0, 2].set_title('Feature Importance')\n",
    "axes[0, 2].set_xticks(range(len(feature_columns)))\n",
    "axes[0, 2].set_xticklabels(feature_names_clean, rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, importance_values):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{value:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Prediction error distribution\n",
    "axes[1, 0].hist(residuals, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics\n",
    "mean_residual = np.mean(residuals)\n",
    "std_residual = np.std(residuals)\n",
    "axes[1, 0].text(0.05, 0.95, f'Mean: ${mean_residual:.0f}\\\\nStd: ${std_residual:.0f}',\n",
    "                transform=axes[1, 0].transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "\n",
    "# 5. Model coefficients visualization\n",
    "coef_colors = ['green' if c > 0 else 'red' for c in coefficients]\n",
    "bars = axes[1, 1].bar(range(len(feature_columns)), coefficients, \n",
    "                      alpha=0.7, color=coef_colors)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Features')\n",
    "axes[1, 1].set_ylabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Model Coefficients (+ = Positive Impact)')\n",
    "axes[1, 1].set_xticks(range(len(feature_columns)))\n",
    "axes[1, 1].set_xticklabels(feature_names_clean, rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, coefficients):\n",
    "    height = bar.get_height()\n",
    "    label_y = height + (abs(height) * 0.1) if height >= 0 else height - (abs(height) * 0.1)\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., label_y,\n",
    "                    f'{value:.0f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "# 6. Training vs Test Performance\n",
    "metrics = ['R¬≤', 'RMSE', 'MAE']\n",
    "train_values = [train_r2, train_rmse/1000, train_mae/1000]  # Scale RMSE and MAE to thousands\n",
    "test_values = [test_r2, test_rmse/1000, test_mae/1000]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 2].bar(x - width/2, train_values, width, label='Training', alpha=0.7)\n",
    "bars2 = axes[1, 2].bar(x + width/2, test_values, width, label='Testing', alpha=0.7)\n",
    "\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Training vs Test Performance')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(['R¬≤', 'RMSE (K$)', 'MAE (K$)'])\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar1, bar2, val1, val2 in zip(bars1, bars2, train_values, test_values):\n",
    "    height1, height2 = bar1.get_height(), bar2.get_height()\n",
    "    axes[1, 2].text(bar1.get_x() + bar1.get_width()/2., height1 + 0.01,\n",
    "                    f'{val1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    axes[1, 2].text(bar2.get_x() + bar2.get_width()/2., height2 + 0.01,\n",
    "                    f'{val2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Comprehensive visualizations created!\")\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(f\"  ‚Ä¢ The model explains {test_r2:.1%} of price variation\")\n",
    "print(f\"  ‚Ä¢ Average prediction error: ${test_mae:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Most important feature: {feature_names_clean[np.argmax(importance_values)]}\")\n",
    "\n",
    "if abs(mean_residual) < test_mae * 0.1:\n",
    "    print(f\"  ‚Ä¢ Residuals are well-centered around zero (unbiased)\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Residuals show some bias (mean = ${mean_residual:.0f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Visualization analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26189bcd",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Key Learnings\n",
    "\n",
    "### üéØ What We Accomplished:\n",
    "1. **Loaded and explored** a multi-feature housing dataset\n",
    "2. **Analyzed correlations** between features and target variable\n",
    "3. **Prepared data** with encoding and feature scaling\n",
    "4. **Trained a multiple linear regression** model with several features\n",
    "5. **Made predictions** and evaluated model performance\n",
    "6. **Visualized results** with comprehensive charts\n",
    "7. **Interpreted coefficients** and feature importance\n",
    "\n",
    "### üìä Key Results:\n",
    "- **R¬≤ Score**: {test_r2:.4f} ({test_r2*100:.1f}% of variance explained)\n",
    "- **RMSE**: ${test_rmse:,.2f} (average prediction error)\n",
    "- **MAE**: ${test_mae:,.2f} (typical absolute error)\n",
    "- **Feature Count**: {len(feature_columns)} features used\n",
    "- **Most Important Feature**: Based on coefficient magnitude\n",
    "\n",
    "### üí° Key Learnings:\n",
    "\n",
    "**Multiple vs Simple Linear Regression:**\n",
    "- **Multiple features** can capture more complex relationships\n",
    "- **Feature scaling** is important when features have different units\n",
    "- **Multicollinearity** can affect coefficient interpretation\n",
    "- **More features** don't always mean better performance\n",
    "\n",
    "**Model Interpretation:**\n",
    "- **Coefficients** show the impact of each feature on the target\n",
    "- **Positive coefficients** increase the prediction\n",
    "- **Negative coefficients** decrease the prediction\n",
    "- **Coefficient magnitude** indicates feature importance\n",
    "\n",
    "**Performance Evaluation:**\n",
    "- **R¬≤ Score** measures explained variance (higher is better)\n",
    "- **RMSE** shows average prediction error in target units\n",
    "- **Residuals analysis** helps identify model issues\n",
    "- **Training vs test** comparison reveals overfitting\n",
    "\n",
    "**Feature Analysis:**\n",
    "- **Correlation analysis** helps understand feature relationships\n",
    "- **Feature scaling** puts all features on similar scales\n",
    "- **Categorical encoding** allows non-numeric features to be used\n",
    "- **Feature importance** guides feature selection\n",
    "\n",
    "### ‚ö†Ô∏è Important Considerations:\n",
    "- **Assumption of linearity**: Multiple regression assumes linear relationships\n",
    "- **Feature independence**: Highly correlated features can cause issues\n",
    "- **Outliers**: Can disproportionately affect the model\n",
    "- **Sample size**: Need enough data relative to number of features\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Learn about **polynomial regression** for non-linear relationships\n",
    "- Explore **regularization** techniques (Ridge, Lasso) to prevent overfitting\n",
    "- Study **logistic regression** for classification problems\n",
    "- Practice **feature engineering** and selection techniques\n",
    "\n",
    "### ü§î Questions to Explore:\n",
    "- How do we handle categorical features with many categories?\n",
    "- What happens when we have more features than samples?\n",
    "- How do we detect and handle multicollinearity?\n",
    "- When should we use polynomial features vs multiple features?\n",
    "\n",
    "### üéâ Congratulations!\n",
    "You've successfully mastered multiple linear regression! You now understand:\n",
    "- How to work with multiple features simultaneously\n",
    "- The importance of data preprocessing and feature scaling\n",
    "- How to interpret model coefficients and feature importance\n",
    "- Various ways to evaluate and visualize model performance\n",
    "\n",
    "This foundation prepares you for more advanced regression techniques and real-world machine learning projects! üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
