{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5248f205",
   "metadata": {},
   "source": [
    "# Logistic Regression Tutorial\n",
    "## Binary Classification: Predicting Expensive Houses\n",
    "\n",
    "Welcome to **Logistic Regression** - our first classification algorithm!\n",
    "\n",
    "### What you'll learn:\n",
    "- How logistic regression differs from linear regression\n",
    "- The sigmoid function and probability prediction\n",
    "- Binary classification metrics (accuracy, precision, recall)\n",
    "- ROC curves and confusion matrices\n",
    "- Decision boundaries and feature importance\n",
    "\n",
    "### Our Task:\n",
    "Predict whether a house is **expensive** (>$350,000) based on its features.\n",
    "\n",
    "Let's start classifying! 🎯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd248068",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data\n",
    "\n",
    "For classification, we need additional metrics and visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a58559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, confusion_matrix, roc_auc_score,\n",
    "                           roc_curve, classification_report)\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\"✅ Libraries imported and data loaded!\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71852736",
   "metadata": {},
   "source": [
    "## Step 2: Explore the Target Variable\n",
    "\n",
    "Let's analyze our binary target variable - whether houses are expensive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable\n",
    "print(\"=\" * 50)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count classes\n",
    "target_counts = data['expensive'].value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(f\"Not Expensive (0): {target_counts[0]} ({target_counts[0]/len(data)*100:.1f}%)\")\n",
    "print(f\"Expensive (1): {target_counts[1]} ({target_counts[1]/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Pie chart\n",
    "labels = ['Not Expensive', 'Expensive']\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "ax1.pie(target_counts.values, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Class Distribution')\n",
    "\n",
    "# Bar chart\n",
    "ax2.bar(labels, target_counts.values, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Class Counts')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if dataset is balanced\n",
    "balance_ratio = min(target_counts) / max(target_counts)\n",
    "if balance_ratio > 0.8:\n",
    "    print(\"✅ Dataset is well balanced\")\n",
    "elif balance_ratio > 0.6:\n",
    "    print(\"👍 Dataset is reasonably balanced\")\n",
    "else:\n",
    "    print(\"⚠️ Dataset is imbalanced - consider balancing techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e4ad1",
   "metadata": {},
   "source": [
    "## Step 3: Feature Analysis by Class\n",
    "\n",
    "Let's analyze how our features differ between expensive and non-expensive houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features by class\n",
    "print(\"=\" * 50)\n",
    "print(\"FEATURE ANALYSIS BY CLASS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "numerical_features = ['area', 'bedrooms', 'age']\n",
    "\n",
    "for feature in numerical_features:\n",
    "    expensive_mean = data[data['expensive'] == 1][feature].mean()\n",
    "    not_expensive_mean = data[data['expensive'] == 0][feature].mean()\n",
    "    \n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Expensive houses: {expensive_mean:.2f}\")\n",
    "    print(f\"  Not expensive houses: {not_expensive_mean:.2f}\")\n",
    "    print(f\"  Difference: {expensive_mean - not_expensive_mean:.2f}\")\n",
    "\n",
    "# Visualize feature distributions by class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Analysis by Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "labels = ['Not Expensive', 'Expensive']\n",
    "\n",
    "# Area distribution by class\n",
    "for i, class_val in enumerate([0, 1]):\n",
    "    class_data = data[data['expensive'] == class_val]['area']\n",
    "    axes[0, 0].hist(class_data, alpha=0.7, label=labels[i], color=colors[i], bins=10)\n",
    "axes[0, 0].set_xlabel('House Area (sq ft)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Area Distribution by Class')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot: Area by class\n",
    "data.boxplot(column='area', by='expensive', ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Expensive (0=No, 1=Yes)')\n",
    "axes[0, 1].set_ylabel('Area (sq ft)')\n",
    "axes[0, 1].set_title('Area Distribution by Class')\n",
    "\n",
    "# Age distribution by class\n",
    "for i, class_val in enumerate([0, 1]):\n",
    "    class_data = data[data['expensive'] == class_val]['age']\n",
    "    axes[1, 0].hist(class_data, alpha=0.7, label=labels[i], color=colors[i], bins=10)\n",
    "axes[1, 0].set_xlabel('House Age (years)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Age Distribution by Class')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Area vs Price, colored by class\n",
    "for i, class_val in enumerate([0, 1]):\n",
    "    class_data = data[data['expensive'] == class_val]\n",
    "    axes[1, 1].scatter(class_data['area'], class_data['price'],\n",
    "                      alpha=0.7, label=labels[i], color=colors[i])\n",
    "axes[1, 1].set_xlabel('Area (sq ft)')\n",
    "axes[1, 1].set_ylabel('Price ($)')\n",
    "axes[1, 1].set_title('Area vs Price (by Class)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d9334",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation for Classification\n",
    "\n",
    "Prepare our features and target for logistic regression, including encoding categorical variables and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for logistic regression\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Handle categorical variables (encode location)\n",
    "label_encoder = LabelEncoder()\n",
    "data_processed = data.copy()\n",
    "data_processed['location_encoded'] = label_encoder.fit_transform(data_processed['location'])\n",
    "\n",
    "print(\"Location encoding:\")\n",
    "for i, location in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {location} -> {i}\")\n",
    "\n",
    "# Select features (excluding price as it's used to create target)\n",
    "feature_columns = ['area', 'bedrooms', 'age', 'location_encoded']\n",
    "X = data_processed[feature_columns].values\n",
    "y = data_processed['expensive'].values\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "# Split the data with stratification (maintains class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Ensures balanced split\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "train_dist = np.bincount(y_train)\n",
    "test_dist = np.bincount(y_test)\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Training: {train_dist} ({train_dist/len(y_train)*100})\")\n",
    "print(f\"Testing: {test_dist} ({test_dist/len(y_test)*100})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1912e3b",
   "metadata": {},
   "source": [
    "## Step 5: Feature Scaling\n",
    "\n",
    "Scale features for optimal logistic regression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88299a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# Show scaling effect\n",
    "print(\"\\nScaling effect (first 3 features):\")\n",
    "for i, feature in enumerate(feature_columns[:3]):\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Original - Mean: {X_train[:, i].mean():.2f}, Std: {X_train[:, i].std():.2f}\")\n",
    "    print(f\"  Scaled   - Mean: {X_train_scaled[:, i].mean():.2f}, Std: {X_train_scaled[:, i].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68dfcd",
   "metadata": {},
   "source": [
    "## Step 6: Train Logistic Regression Model\n",
    "\n",
    "Train our logistic regression classifier and analyze the learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# Extract model parameters\n",
    "coefficients = model.coef_[0]\n",
    "intercept = model.intercept_[0]\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feature, coef in zip(feature_columns, coefficients):\n",
    "    print(f\"  {feature:15s}: {coef:8.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = list(zip(feature_columns, np.abs(coefficients)))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n📊 Feature Importance (coefficient magnitudes):\")\n",
    "for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "    print(f\"  {i}. {feature:15s}: {importance:.4f}\")\n",
    "\n",
    "# Coefficient interpretation\n",
    "print(f\"\\n🔍 Coefficient Interpretation (Odds Ratios):\")\n",
    "for feature, coef in zip(feature_columns, coefficients):\n",
    "    odds_ratio = np.exp(coef)\n",
    "    if coef > 0:\n",
    "        effect = \"increases\"\n",
    "    else:\n",
    "        effect = \"decreases\"\n",
    "    print(f\"  {feature:15s}: {effect} odds by factor of {odds_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57ebc6",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions and Get Probabilities\n",
    "\n",
    "Use our trained model to make predictions and get probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"✅ Predictions completed on {len(y_pred)} test samples!\")\n",
    "print(f\"Probability range: {y_prob.min():.3f} to {y_prob.max():.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n📋 Sample Predictions:\")\n",
    "print(f\"{'Actual':>8} {'Predicted':>10} {'Probability':>12} {'Confidence':>12}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(10, len(y_test))):\n",
    "    actual = y_test[i]\n",
    "    predicted = y_pred[i]\n",
    "    probability = y_prob[i]\n",
    "    confidence = max(probability, 1 - probability)\n",
    "    \n",
    "    print(f\"{actual:7d} {predicted:9d} {probability:11.3f} {confidence:11.3f}\")\n",
    "\n",
    "# Prediction statistics\n",
    "pred_dist = np.bincount(y_pred)\n",
    "print(f\"\\n📊 Prediction Distribution:\")\n",
    "print(f\"Predicted Not Expensive: {pred_dist[0]}\")\n",
    "print(f\"Predicted Expensive: {pred_dist[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23019a",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "Evaluate our classification model using comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b13127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"📊 Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision*100:.1f}%)\")\n",
    "print(f\"  Recall:    {recall:.4f} ({recall*100:.1f}%)\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\n📈 Confusion Matrix:\")\n",
    "print(f\"              Predicted\")\n",
    "print(f\"           Not Exp  Expensive\")\n",
    "print(f\"Actual Not Exp  {tn:3d}      {fp:3d}\")\n",
    "print(f\"    Expensive   {fn:3d}      {tp:3d}\")\n",
    "\n",
    "print(f\"\\n🔍 Detailed Breakdown:\")\n",
    "print(f\"  True Positives (TP):  {tp} - Correctly predicted expensive\")\n",
    "print(f\"  True Negatives (TN):  {tn} - Correctly predicted not expensive\")\n",
    "print(f\"  False Positives (FP): {fp} - Incorrectly predicted expensive\")\n",
    "print(f\"  False Negatives (FN): {fn} - Incorrectly predicted not expensive\")\n",
    "\n",
    "# Performance interpretation\n",
    "print(f\"\\n💡 Performance Interpretation:\")\n",
    "if accuracy > 0.9:\n",
    "    print(\"🌟 Excellent accuracy!\")\n",
    "elif accuracy > 0.8:\n",
    "    print(\"✅ Good accuracy!\")\n",
    "elif accuracy > 0.7:\n",
    "    print(\"👍 Fair accuracy.\")\n",
    "else:\n",
    "    print(\"⚠️ Poor accuracy - model needs improvement.\")\n",
    "\n",
    "if roc_auc > 0.9:\n",
    "    print(\"🌟 Excellent discrimination ability!\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"✅ Good discrimination ability!\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"👍 Fair discrimination ability.\")\n",
    "else:\n",
    "    print(\"⚠️ Poor discrimination.\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📋 Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Expensive', 'Expensive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb5663",
   "metadata": {},
   "source": [
    "## Step 9: Visualization of Results\n",
    "\n",
    "Create comprehensive visualizations to understand our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ee770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Logistic Regression Classification Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['Not Expensive', 'Expensive'],\n",
    "            yticklabels=['Not Expensive', 'Expensive'])\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Probability Distribution by Class\n",
    "prob_expensive = y_prob[y_test == 1]\n",
    "prob_not_expensive = y_prob[y_test == 0]\n",
    "\n",
    "axes[0, 2].hist(prob_not_expensive, alpha=0.7, label='Not Expensive', \n",
    "                color='lightcoral', bins=15, density=True)\n",
    "axes[0, 2].hist(prob_expensive, alpha=0.7, label='Expensive', \n",
    "                color='lightblue', bins=15, density=True)\n",
    "axes[0, 2].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[0, 2].set_xlabel('Predicted Probability')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Probability Distribution by Class')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance\n",
    "coefficients_abs = np.abs(coefficients)\n",
    "feature_names = feature_columns\n",
    "\n",
    "importance_order = np.argsort(coefficients_abs)[::-1]\n",
    "sorted_features = [feature_names[i] for i in importance_order]\n",
    "sorted_coefficients = coefficients_abs[importance_order]\n",
    "\n",
    "bars = axes[1, 0].bar(range(len(sorted_features)), sorted_coefficients, \n",
    "                      alpha=0.7, color=['blue', 'green', 'red', 'orange'])\n",
    "axes[1, 0].set_xlabel('Features')\n",
    "axes[1, 0].set_ylabel('Coefficient Magnitude')\n",
    "axes[1, 0].set_title('Feature Importance')\n",
    "axes[1, 0].set_xticks(range(len(sorted_features)))\n",
    "axes[1, 0].set_xticklabels(sorted_features, rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Prediction Confidence\n",
    "confidence = np.maximum(y_prob, 1 - y_prob)\n",
    "correct_predictions = (y_pred == y_test)\n",
    "\n",
    "axes[1, 1].scatter(confidence[correct_predictions], [1]*sum(correct_predictions), \n",
    "                   alpha=0.6, color='green', label='Correct', s=30)\n",
    "axes[1, 1].scatter(confidence[~correct_predictions], [0]*sum(~correct_predictions), \n",
    "                   alpha=0.6, color='red', label='Incorrect', s=30)\n",
    "axes[1, 1].set_xlabel('Prediction Confidence')\n",
    "axes[1, 1].set_ylabel('Prediction Outcome')\n",
    "axes[1, 1].set_title('Prediction Confidence vs Accuracy')\n",
    "axes[1, 1].set_yticks([0, 1])\n",
    "axes[1, 1].set_yticklabels(['Incorrect', 'Correct'])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Metrics Comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "values = [accuracy, precision, recall, f1, roc_auc]\n",
    "colors_metric = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "\n",
    "bars = axes[1, 2].bar(metrics, values, alpha=0.7, color=colors_metric)\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Classification Metrics Summary')\n",
    "axes[1, 2].set_ylim([0, 1])\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3ae2b",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Key Learnings\n",
    "\n",
    "### 🎯 What We Accomplished:\n",
    "1. **Converted regression to classification** by creating binary target\n",
    "2. **Analyzed class distributions** and feature differences\n",
    "3. **Prepared data** with encoding and scaling\n",
    "4. **Trained logistic regression** classifier\n",
    "5. **Made probability predictions** on test data\n",
    "6. **Evaluated performance** with classification metrics\n",
    "7. **Visualized results** with ROC curves and confusion matrices\n",
    "\n",
    "### 📊 Key Results:\n",
    "- **Accuracy**: {accuracy:.1%} of predictions correct\n",
    "- **ROC-AUC**: {roc_auc:.3f} discrimination ability\n",
    "- **Precision**: {precision:.1%} of expensive predictions correct\n",
    "- **Recall**: {recall:.1%} of expensive houses identified\n",
    "\n",
    "### 💡 Key Learnings:\n",
    "- **Logistic regression** uses sigmoid function for probability prediction\n",
    "- **Classification metrics** are different from regression metrics\n",
    "- **ROC-AUC** measures model's ability to distinguish classes\n",
    "- **Confusion matrix** provides detailed breakdown of predictions\n",
    "- **Feature scaling** is important for logistic regression\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "- Explore **polynomial regression** for non-linear relationships\n",
    "- Learn about **regularization** (Ridge, Lasso) for overfitting\n",
    "- Try **multi-class classification** problems\n",
    "- Experiment with **feature engineering** and selection\n",
    "\n",
    "### 🤔 Questions to Consider:\n",
    "- How would different probability thresholds affect results?\n",
    "- What if we had more than 2 classes?\n",
    "- How do we handle highly imbalanced datasets?\n",
    "- When is classification better than regression?\n",
    "\n",
    "Congratulations on mastering logistic regression! 🎉"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
