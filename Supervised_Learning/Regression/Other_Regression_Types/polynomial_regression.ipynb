{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531ced98",
   "metadata": {},
   "source": [
    "# Polynomial Regression Tutorial\n",
    "## Capturing Non-Linear Relationships\n",
    "\n",
    "Welcome to **Polynomial Regression** - where we go beyond straight lines!\n",
    "\n",
    "### What you'll learn:\n",
    "- When linear regression isn't enough\n",
    "- How to create polynomial features\n",
    "- Finding the optimal polynomial degree\n",
    "- Bias-variance tradeoff and overfitting\n",
    "- Cross-validation for model selection\n",
    "- Regularization techniques\n",
    "\n",
    "### Our Challenge:\n",
    "Model house prices with **curved relationships** that simple linear regression can't capture.\n",
    "\n",
    "Let's capture those curves! üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e261d0",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data\n",
    "\n",
    "For polynomial regression, we need additional tools for feature transformation and model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\"‚úÖ Libraries imported and data loaded!\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba459f60",
   "metadata": {},
   "source": [
    "## Step 2: Explore Non-Linear Relationships\n",
    "\n",
    "Let's investigate potential non-linear patterns in our data that polynomial regression could capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots to identify non-linear relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Exploring Non-Linear Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Area vs Price\n",
    "axes[0, 0].scatter(data['area'], data['price'], alpha=0.6, color='blue')\n",
    "axes[0, 0].set_xlabel('Area (sq ft)')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].set_title('Area vs Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Age vs Price (might show non-linear depreciation)\n",
    "axes[0, 1].scatter(data['age'], data['price'], alpha=0.6, color='green')\n",
    "axes[0, 1].set_xlabel('Age (years)')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "axes[0, 1].set_title('Age vs Price')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bedrooms vs Price\n",
    "axes[1, 0].scatter(data['bedrooms'], data['price'], alpha=0.6, color='red')\n",
    "axes[1, 0].set_xlabel('Bedrooms')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].set_title('Bedrooms vs Price')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Create a synthetic non-linear feature: Area squared relationship\n",
    "area_squared = data['area'] ** 2\n",
    "axes[1, 1].scatter(area_squared, data['price'], alpha=0.6, color='purple')\n",
    "axes[1, 1].set_xlabel('Area Squared (sq ft)¬≤')\n",
    "axes[1, 1].set_ylabel('Price ($)')\n",
    "axes[1, 1].set_title('Area¬≤ vs Price')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlations with polynomial features\n",
    "print(\"=\" * 50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correlations = {\n",
    "    'area (linear)': np.corrcoef(data['area'], data['price'])[0, 1],\n",
    "    'area¬≤ (quadratic)': np.corrcoef(data['area']**2, data['price'])[0, 1],\n",
    "    'area¬≥ (cubic)': np.corrcoef(data['area']**3, data['price'])[0, 1],\n",
    "    'age (linear)': np.corrcoef(data['age'], data['price'])[0, 1],\n",
    "    'age¬≤ (quadratic)': np.corrcoef(data['age']**2, data['price'])[0, 1]\n",
    "}\n",
    "\n",
    "for feature, corr in correlations.items():\n",
    "    print(f\"{feature:20s}: {corr:7.4f}\")\n",
    "\n",
    "# Find the strongest correlation\n",
    "best_feature = max(correlations.items(), key=lambda x: abs(x[1]))\n",
    "print(f\"\\nüéØ Strongest correlation: {best_feature[0]} ({best_feature[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba464f71",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation\n",
    "\n",
    "Prepare our data for polynomial regression, focusing on the most promising features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c67e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for polynomial regression\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Handle categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data_processed = data.copy()\n",
    "data_processed['location_encoded'] = label_encoder.fit_transform(data['location'])\n",
    "\n",
    "# Select features\n",
    "feature_columns = ['area', 'bedrooms', 'age', 'location_encoded']\n",
    "X = data_processed[feature_columns]\n",
    "y = data_processed['price']\n",
    "\n",
    "print(f\"Features: {feature_columns}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(\"‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c94f60",
   "metadata": {},
   "source": [
    "## Step 4: Linear Regression Baseline\n",
    "\n",
    "First, let's establish a baseline with simple linear regression to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446983ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression baseline\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LINEAR REGRESSION BASELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Squared Error (MSE):  ${mse_linear:,.2f}\")\n",
    "print(f\"Root Mean Squared Error:   ${rmse_linear:,.2f}\")\n",
    "print(f\"Mean Absolute Error:       ${mae_linear:,.2f}\")\n",
    "print(f\"R¬≤ Score:                  {r2_linear:.4f}\")\n",
    "print(f\"Explained Variance:        {r2_linear*100:.1f}%\")\n",
    "\n",
    "# Store baseline results for comparison\n",
    "baseline_results = {\n",
    "    'model_name': 'Linear Regression',\n",
    "    'degree': 1,\n",
    "    'mse': mse_linear,\n",
    "    'rmse': rmse_linear,\n",
    "    'r2': r2_linear,\n",
    "    'mae': mae_linear\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Baseline established!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff091e4",
   "metadata": {},
   "source": [
    "## Step 5: Polynomial Feature Creation\n",
    "\n",
    "Transform our features into polynomial features of different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3126d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different polynomial degrees\n",
    "degrees = [2, 3, 4, 5]\n",
    "polynomial_results = []\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"POLYNOMIAL FEATURE EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"\\nüîç Testing Polynomial Degree {degree}:\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    \n",
    "    print(f\"  Original features: {X_train.shape[1]}\")\n",
    "    print(f\"  Polynomial features: {X_train_poly.shape[1]}\")\n",
    "    print(f\"  Feature expansion: {X_train_poly.shape[1] / X_train.shape[1]:.1f}x\")\n",
    "    \n",
    "    # Train polynomial regression\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_poly = poly_model.predict(X_test_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "    rmse_poly = np.sqrt(mse_poly)\n",
    "    r2_poly = r2_score(y_test, y_pred_poly)\n",
    "    mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
    "    \n",
    "    print(f\"  RMSE: ${rmse_poly:,.2f}\")\n",
    "    print(f\"  R¬≤: {r2_poly:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    polynomial_results.append({\n",
    "        'model_name': f'Polynomial (degree {degree})',\n",
    "        'degree': degree,\n",
    "        'mse': mse_poly,\n",
    "        'rmse': rmse_poly,\n",
    "        'r2': r2_poly,\n",
    "        'mae': mae_poly,\n",
    "        'num_features': X_train_poly.shape[1],\n",
    "        'model': poly_model,\n",
    "        'poly_features': poly_features\n",
    "    })\n",
    "    \n",
    "    # Check for improvement\n",
    "    improvement = r2_poly - r2_linear\n",
    "    if improvement > 0:\n",
    "        print(f\"  ‚úÖ Improvement: +{improvement:.4f} R¬≤\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Worse: {improvement:.4f} R¬≤\")\n",
    "\n",
    "print(\"\\n‚úÖ Polynomial exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b071645",
   "metadata": {},
   "source": [
    "## Step 6: Cross-Validation for Model Selection\n",
    "\n",
    "Use cross-validation to find the optimal polynomial degree and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b20baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for different polynomial degrees\n",
    "degrees_cv = range(1, 8)\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "train_scores_mean = []\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CROSS-VALIDATION FOR MODEL SELECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_combined = pd.concat([X_train, X_test])\n",
    "y_combined = pd.concat([y_train, y_test])\n",
    "\n",
    "for degree in degrees_cv:\n",
    "    # Create pipeline with polynomial features and linear regression\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation scores (validation)\n",
    "    cv_scores = cross_val_score(pipeline, X_combined, y_combined, \n",
    "                               cv=5, scoring='r2')\n",
    "    cv_scores_mean.append(cv_scores.mean())\n",
    "    cv_scores_std.append(cv_scores.std())\n",
    "    \n",
    "    # Training scores (to detect overfitting)\n",
    "    pipeline.fit(X_combined, y_combined)\n",
    "    train_score = pipeline.score(X_combined, y_combined)\n",
    "    train_scores_mean.append(train_score)\n",
    "    \n",
    "    print(f\"Degree {degree}: CV R¬≤ = {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f}), \"\n",
    "          f\"Train R¬≤ = {train_score:.4f}\")\n",
    "\n",
    "# Find optimal degree\n",
    "optimal_degree = degrees_cv[np.argmax(cv_scores_mean)]\n",
    "best_cv_score = max(cv_scores_mean)\n",
    "\n",
    "print(f\"\\nüéØ Optimal Degree: {optimal_degree}\")\n",
    "print(f\"üéØ Best CV R¬≤: {best_cv_score:.4f}\")\n",
    "\n",
    "# Plot validation curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(degrees_cv, cv_scores_mean, 'o-', color='blue', label='Validation Score')\n",
    "plt.fill_between(degrees_cv, \n",
    "                 np.array(cv_scores_mean) - np.array(cv_scores_std),\n",
    "                 np.array(cv_scores_mean) + np.array(cv_scores_std),\n",
    "                 alpha=0.2, color='blue')\n",
    "plt.plot(degrees_cv, train_scores_mean, 'o-', color='red', label='Training Score')\n",
    "plt.axvline(x=optimal_degree, color='green', linestyle='--', label=f'Optimal Degree ({optimal_degree})')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Validation Curve: Polynomial Degree vs Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot overfitting detection\n",
    "plt.subplot(1, 2, 2)\n",
    "gap = np.array(train_scores_mean) - np.array(cv_scores_mean)\n",
    "plt.plot(degrees_cv, gap, 'o-', color='orange', linewidth=2)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=optimal_degree, color='green', linestyle='--', label=f'Optimal Degree ({optimal_degree})')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Training R¬≤ - Validation R¬≤')\n",
    "plt.title('Overfitting Detection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze overfitting\n",
    "print(f\"\\nüìä Overfitting Analysis:\")\n",
    "for i, degree in enumerate(degrees_cv):\n",
    "    gap = train_scores_mean[i] - cv_scores_mean[i]\n",
    "    if gap < 0.05:\n",
    "        status = \"‚úÖ Good fit\"\n",
    "    elif gap < 0.1:\n",
    "        status = \"‚ö†Ô∏è Slight overfitting\"\n",
    "    else:\n",
    "        status = \"‚ùå Overfitting\"\n",
    "    print(f\"  Degree {degree}: Gap = {gap:.4f} - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd6a9b",
   "metadata": {},
   "source": [
    "## Step 7: Train Final Polynomial Model\n",
    "\n",
    "Train the final model using the optimal polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final polynomial model with optimal degree\n",
    "print(\"=\" * 50)\n",
    "print(f\"FINAL POLYNOMIAL MODEL (DEGREE {optimal_degree})\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create polynomial features with optimal degree\n",
    "final_poly_features = PolynomialFeatures(degree=optimal_degree, include_bias=False)\n",
    "X_train_final = final_poly_features.fit_transform(X_train)\n",
    "X_test_final = final_poly_features.transform(X_test)\n",
    "\n",
    "print(f\"Feature transformation:\")\n",
    "print(f\"  Original features: {X_train.shape[1]}\")\n",
    "print(f\"  Polynomial features: {X_train_final.shape[1]}\")\n",
    "print(f\"  Feature names (first 10): {final_poly_features.get_feature_names_out(feature_columns)[:10]}\")\n",
    "\n",
    "# Scale features for better numerical stability\n",
    "scaler = StandardScaler()\n",
    "X_train_final_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_final_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# Train final model\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_final = final_model.predict(X_test_final_scaled)\n",
    "\n",
    "# Evaluate final model\n",
    "mse_final = mean_squared_error(y_test, y_pred_final)\n",
    "rmse_final = np.sqrt(mse_final)\n",
    "r2_final = r2_score(y_test, y_pred_final)\n",
    "mae_final = mean_absolute_error(y_test, y_pred_final)\n",
    "\n",
    "print(f\"\\nüìä Final Model Performance:\")\n",
    "print(f\"  Mean Squared Error:   ${mse_final:,.2f}\")\n",
    "print(f\"  Root Mean Squared Error: ${rmse_final:,.2f}\")\n",
    "print(f\"  Mean Absolute Error:  ${mae_final:,.2f}\")\n",
    "print(f\"  R¬≤ Score:             {r2_final:.4f}\")\n",
    "print(f\"  Explained Variance:   {r2_final*100:.1f}%\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = r2_final - r2_linear\n",
    "print(f\"\\nüéØ Improvement over Linear Regression:\")\n",
    "print(f\"  R¬≤ improvement: +{improvement:.4f} ({improvement/r2_linear*100:+.1f}%)\")\n",
    "print(f\"  RMSE improvement: ${rmse_linear - rmse_final:,.2f}\")\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print(\"  ‚úÖ Significant improvement with polynomial features!\")\n",
    "elif improvement > 0:\n",
    "    print(\"  üëç Modest improvement with polynomial features.\")\n",
    "else:\n",
    "    print(\"  ‚ùå No improvement - linear model is sufficient.\")\n",
    "\n",
    "print(\"\\n‚úÖ Final model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849de07",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance Analysis\n",
    "\n",
    "Analyze which polynomial features contribute most to our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b38e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "print(\"=\" * 50)\n",
    "print(\"POLYNOMIAL FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_names = final_poly_features.get_feature_names_out(feature_columns)\n",
    "coefficients = final_model.coef_\n",
    "\n",
    "# Create feature importance (absolute coefficients)\n",
    "feature_importance = list(zip(feature_names, np.abs(coefficients)))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Top 10 Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance[:10], 1):\n",
    "    print(f\"  {i:2d}. {feature:25s}: {importance:10.2f}\")\n",
    "\n",
    "# Visualize top features\n",
    "top_features = feature_importance[:15]\n",
    "features, importances = zip(*top_features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(features)), importances, alpha=0.7)\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Coefficient Magnitude')\n",
    "plt.title(f'Top 15 Polynomial Feature Importance (Degree {optimal_degree})')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Color bars by feature type\n",
    "for i, bar in enumerate(bars):\n",
    "    if 'area' in features[i]:\n",
    "        bar.set_color('blue')\n",
    "    elif 'age' in features[i]:\n",
    "        bar.set_color('red')\n",
    "    elif 'bedrooms' in features[i]:\n",
    "        bar.set_color('green')\n",
    "    else:\n",
    "        bar.set_color('orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze feature types\n",
    "feature_types = {'area': 0, 'age': 0, 'bedrooms': 0, 'location': 0, 'interaction': 0}\n",
    "\n",
    "for feature, importance in feature_importance:\n",
    "    if feature.count(' ') > 0:  # Interaction terms\n",
    "        feature_types['interaction'] += importance\n",
    "    elif 'area' in feature:\n",
    "        feature_types['area'] += importance\n",
    "    elif 'age' in feature:\n",
    "        feature_types['age'] += importance\n",
    "    elif 'bedrooms' in feature:\n",
    "        feature_types['bedrooms'] += importance\n",
    "    elif 'location' in feature:\n",
    "        feature_types['location'] += importance\n",
    "\n",
    "print(f\"\\nüìä Feature Type Importance:\")\n",
    "for feature_type, total_importance in sorted(feature_types.items(), \n",
    "                                           key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feature_type:12s}: {total_importance:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfd9cc",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Results\n",
    "\n",
    "Create comprehensive visualizations to compare linear vs polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Polynomial vs Linear Regression Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Actual vs Predicted (Linear)\n",
    "axes[0, 0].scatter(y_test, y_pred_linear, alpha=0.6, color='blue')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Price ($)')\n",
    "axes[0, 0].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 0].set_title(f'Linear Regression (R¬≤ = {r2_linear:.3f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted (Polynomial)\n",
    "axes[0, 1].scatter(y_test, y_pred_final, alpha=0.6, color='green')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Price ($)')\n",
    "axes[0, 1].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 1].set_title(f'Polynomial Regression (R¬≤ = {r2_final:.3f})')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals Comparison\n",
    "residuals_linear = y_test - y_pred_linear\n",
    "residuals_poly = y_test - y_pred_final\n",
    "\n",
    "axes[0, 2].scatter(y_pred_linear, residuals_linear, alpha=0.6, color='blue', label='Linear')\n",
    "axes[0, 2].scatter(y_pred_final, residuals_poly, alpha=0.6, color='green', label='Polynomial')\n",
    "axes[0, 2].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 2].set_xlabel('Predicted Price ($)')\n",
    "axes[0, 2].set_ylabel('Residuals ($)')\n",
    "axes[0, 2].set_title('Residuals Analysis')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Complexity vs Performance\n",
    "all_results = [baseline_results] + polynomial_results\n",
    "degrees_plot = [result['degree'] for result in all_results]\n",
    "r2_scores = [result['r2'] for result in all_results]\n",
    "num_features = [X_train.shape[1] if result['degree'] == 1 \n",
    "                else result['num_features'] for result in all_results]\n",
    "\n",
    "axes[1, 0].plot(degrees_plot, r2_scores, 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 0].axvline(x=optimal_degree, color='red', linestyle='--', \n",
    "                  label=f'Optimal Degree ({optimal_degree})')\n",
    "axes[1, 0].set_xlabel('Polynomial Degree')\n",
    "axes[1, 0].set_ylabel('R¬≤ Score')\n",
    "axes[1, 0].set_title('Model Complexity vs Performance')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Feature Count vs Performance\n",
    "axes[1, 1].scatter(num_features, r2_scores, s=100, alpha=0.7)\n",
    "for i, (x, y, degree) in enumerate(zip(num_features, r2_scores, degrees_plot)):\n",
    "    axes[1, 1].annotate(f'Deg {degree}', (x, y), xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=8)\n",
    "axes[1, 1].set_xlabel('Number of Features')\n",
    "axes[1, 1].set_ylabel('R¬≤ Score')\n",
    "axes[1, 1].set_title('Feature Count vs Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance Metrics Comparison\n",
    "metrics = ['R¬≤', 'RMSE ($000s)', 'MAE ($000s)']\n",
    "linear_values = [r2_linear, rmse_linear/1000, mae_linear/1000]\n",
    "poly_values = [r2_final, rmse_final/1000, mae_final/1000]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 2].bar(x - width/2, linear_values, width, label='Linear', alpha=0.7)\n",
    "bars2 = axes[1, 2].bar(x + width/2, poly_values, width, label='Polynomial', alpha=0.7)\n",
    "\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Performance Metrics Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(metrics)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar1, bar2, val1, val2 in zip(bars1, bars2, linear_values, poly_values):\n",
    "    height1, height2 = bar1.get_height(), bar2.get_height()\n",
    "    axes[1, 2].text(bar1.get_x() + bar1.get_width()/2., height1 + 0.01,\n",
    "                    f'{val1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    axes[1, 2].text(bar2.get_x() + bar2.get_width()/2., height2 + 0.01,\n",
    "                    f'{val2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e330b70",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Key Insights\n",
    "\n",
    "### üéØ What We Accomplished:\n",
    "1. **Explored non-linear relationships** in our housing data\n",
    "2. **Created polynomial features** of different degrees\n",
    "3. **Used cross-validation** to find optimal model complexity\n",
    "4. **Detected and avoided overfitting** through validation curves\n",
    "5. **Compared linear vs polynomial** regression performance\n",
    "6. **Analyzed feature importance** in polynomial space\n",
    "\n",
    "### üìä Key Results:\n",
    "- **Optimal Polynomial Degree**: {optimal_degree}\n",
    "- **Performance Improvement**: {improvement:+.4f} R¬≤ over linear regression\n",
    "- **Final R¬≤ Score**: {r2_final:.4f} ({r2_final*100:.1f}% variance explained)\n",
    "- **RMSE Reduction**: ${rmse_linear - rmse_final:,.2f}\n",
    "\n",
    "### üí° Key Learnings:\n",
    "\n",
    "**When to Use Polynomial Regression:**\n",
    "- Data shows **curved relationships**\n",
    "- Linear models **underfit** the data\n",
    "- Domain knowledge suggests **non-linear effects**\n",
    "\n",
    "**The Bias-Variance Tradeoff:**\n",
    "- **Low degree**: High bias, low variance (underfitting)\n",
    "- **High degree**: Low bias, high variance (overfitting)\n",
    "- **Optimal degree**: Best balance for generalization\n",
    "\n",
    "**Cross-Validation Benefits:**\n",
    "- **Prevents overfitting** to training data\n",
    "- **Guides model selection** objectively\n",
    "- **Estimates generalization** performance\n",
    "\n",
    "**Feature Explosion:**\n",
    "- Polynomial features **grow rapidly** with degree\n",
    "- **Feature scaling** becomes crucial\n",
    "- **Regularization** may be needed for high degrees\n",
    "\n",
    "### ‚ö†Ô∏è Potential Issues:\n",
    "- **Overfitting** with high-degree polynomials\n",
    "- **Numerical instability** with many features\n",
    "- **Extrapolation problems** outside training range\n",
    "- **Interpretability loss** with complex features\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Learn about **Ridge and Lasso regression** for regularization\n",
    "- Explore **interaction features** and **feature selection**\n",
    "- Try **spline regression** for piecewise polynomials\n",
    "- Study **kernel methods** for non-linear relationships\n",
    "\n",
    "### ü§î Questions to Explore:\n",
    "- How do we handle polynomial regression with many features?\n",
    "- When is polynomial regression better than other non-linear methods?\n",
    "- How do we interpret complex polynomial coefficients?\n",
    "- What's the relationship between polynomial regression and neural networks?\n",
    "\n",
    "Great job mastering polynomial regression and the bias-variance tradeoff! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
